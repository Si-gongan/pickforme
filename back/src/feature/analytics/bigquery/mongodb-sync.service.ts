import dotenv from 'dotenv';
dotenv.config({ path: '.env.local' });

import { bigqueryClient } from './bigquery-client';
import { TABLE_SCHEMAS } from './table-schemas';
import db from 'models';
import { log } from '../../../utils/logger/logger';

export class MongodbSyncService {
  private readonly BATCH_SIZE = 1000;

  private readonly DATASET_ID = process.env.GA4_DATASET_FOUNDATION_ID!;

  /**
   * í…Œì´ë¸” ìë™ ìƒì„± í•¨ìˆ˜
   */
  private async ensureTableExists(tableName: string) {
    try {
      console.log(`ğŸ” Checking table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });

      const table = dataset.table(tableName);
      const [exists] = await table.exists();

      if (!exists) {
        const schema = TABLE_SCHEMAS[tableName];
        if (!schema) {
          throw new Error(`Schema not found for table: ${tableName}`);
        }

        console.log(`ğŸ—ï¸ Creating table ${this.DATASET_ID}.${tableName} with schema...`);
        await table.create({
          schema: schema,
          location: 'asia-northeast3',
        });
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} created successfully`);
      } else {
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} already exists`);
      }
    } catch (error) {
      console.error(`âŒ Failed to create table ${this.DATASET_ID}.${tableName}:`, error);
      throw error;
    }
  }

  /**
   * í…Œì´ë¸” ë°ì´í„°ë§Œ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
   */
  private async clearTableData(tableName: string) {
    try {
      console.log(`ğŸ—‘ï¸ Clearing data from table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      // í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ (TRUNCATE)
      await table.delete();
      console.log(`âœ… Cleared data from table ${this.DATASET_ID}.${tableName}`);

      // í…Œì´ë¸” ì¬ìƒì„± (ë¹ˆ í…Œì´ë¸”)
      const schema = TABLE_SCHEMAS[tableName];
      if (!schema) {
        throw new Error(`Schema not found for table: ${tableName}`);
      }

      await table.create({
        schema: schema,
        location: 'asia-northeast3',
      });
      console.log(`âœ… Recreated empty table ${this.DATASET_ID}.${tableName}`);
    } catch (error) {
      console.error(`âŒ Failed to clear table ${tableName}:`, error);
      throw error;
    }
  }

  /**
   * ëª¨ë“  ë°ì´í„°ë¥¼ ì¦ë¶„ ë™ê¸°í™”
   */
  async syncAllData() {
    try {
      log.info('MongoDB ë™ê¸°í™” ì‹œì‘', 'SCHEDULER', 'LOW');

      // í…Œì´ë¸” ì¡´ì¬ í™•ì¸
      console.log('ğŸ”„ Ensuring tables exist...');
      await this.ensureTableExists('users');
      await this.ensureTableExists('purchases');
      await this.ensureTableExists('purchase_failures');

      await this.clearTableData('users');
      await this.clearTableData('purchases');
      await this.clearTableData('purchase_failures');

      const lastSync = await this.getLastSyncTime();

      await Promise.all([
        this.syncUsers(lastSync),
        this.syncPurchases(lastSync),
        this.syncPurchaseFailures(lastSync),
      ]);

      await this.updateLastSyncTime(new Date());

      log.info('MongoDB ë™ê¸°í™” ì™„ë£Œ', 'SCHEDULER', 'LOW');
    } catch (error) {
      void log.error('MongoDB ë™ê¸°í™” ì‹¤íŒ¨', 'SCHEDULER', 'HIGH', { error });
      throw error;
    }
  }

  /**
   * ìœ ì € ë°ì´í„° ë™ê¸°í™”
   */
  private async syncUsers(lastSyncTime?: Date) {
    const query = lastSyncTime ? { updatedAt: { $gt: lastSyncTime } } : {};

    let skip = 0;
    let hasMore = true;

    while (hasMore) {
      const users = await db.User.find(query).skip(skip).limit(this.BATCH_SIZE).lean();

      if (users.length === 0) {
        hasMore = false;
        break;
      }

      // BigQueryì— ë§ëŠ” í˜•íƒœë¡œ ë³€í™˜ (íƒ€ì… ë°©ì–´ ë¡œì§ ì¶”ê°€)
      const transformedUsers = users.map((user) => ({
        _id: user._id.toString(),
        email: user.email,
        point: Number(user.point) || 0,
        aiPoint: Number(user.aiPoint) || 0,
        level: Number(user.level) || 1,
        lastLoginAt: user.lastLoginAt?.toISOString() || null,
        MembershipAt: user.MembershipAt?.toISOString() || null,
        lastMembershipAt: user.lastMembershipAt?.toISOString() || null,
        event: user.event || null,
        createdAt: user.createdAt?.toISOString() || null,
        updatedAt: user.updatedAt?.toISOString() || null,
      }));

      await this.insertBatchToBigQuery('users', transformedUsers);
      skip += this.BATCH_SIZE;
    }

    log.info(`ìœ ì € ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (${skip}ê°œ ë ˆì½”ë“œ)`, 'SCHEDULER', 'LOW');
  }

  /**
   * êµ¬ë§¤ ë°ì´í„° ë™ê¸°í™”
   */
  private async syncPurchases(lastSyncTime?: Date) {
    const query = lastSyncTime ? { updatedAt: { $gt: lastSyncTime } } : {};

    let skip = 0;
    let hasMore = true;

    while (hasMore) {
      const purchases = await db.Purchase.find(query).skip(skip).limit(this.BATCH_SIZE).lean();

      if (purchases.length === 0) {
        hasMore = false;
        break;
      }

      // BigQueryì— ë§ëŠ” í˜•íƒœë¡œ ë³€í™˜
      const transformedPurchases = purchases.map((purchase) => ({
        _id: purchase._id.toString(),
        userId: purchase.userId.toString(),
        productId: purchase.product?.productId || null,
        platform: purchase.product?.platform || null,
        type: purchase.product?.type || null,
        isExpired: purchase.isExpired || false,
        createdAt: purchase.createdAt.toISOString(),
        updatedAt: purchase.updatedAt.toISOString(),
      }));

      await this.insertBatchToBigQuery('purchases', transformedPurchases);
      skip += this.BATCH_SIZE;
    }

    log.info(`êµ¬ë§¤ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (${skip}ê°œ ë ˆì½”ë“œ)`, 'SCHEDULER', 'LOW');
  }

  /**
   * êµ¬ë§¤ ì‹¤íŒ¨ ë°ì´í„° ë™ê¸°í™”
   */
  private async syncPurchaseFailures(lastSyncTime?: Date) {
    const query = lastSyncTime ? { updatedAt: { $gt: lastSyncTime } } : {};

    let skip = 0;
    let hasMore = true;

    while (hasMore) {
      const failures = await db.PurchaseFailure.find(query)
        .skip(skip)
        .limit(this.BATCH_SIZE)
        .lean();

      if (failures.length === 0) {
        hasMore = false;
        break;
      }

      // BigQueryì— ë§ëŠ” í˜•íƒœë¡œ ë³€í™˜
      const transformedFailures = failures.map((failure) => {
        // stringify í•˜ê¸° ì „ì— ê°ì²´ì¸ì§€ í™•ì¸
        const stringifyIfObject = (data: any) =>
          data && typeof data === 'object' ? JSON.stringify(data) : null;

        return {
          _id: failure._id.toString(),
          userId: failure.userId?.toString() || null,
          productId: failure.productId?.toString() || null,
          status: failure.status || null,
          platform: failure.platform || null,
          createdAt: failure.createdAt.toISOString(),
          updatedAt: failure.updatedAt.toISOString(),
        };
      });

      await this.insertBatchToBigQuery('purchase_failures', transformedFailures);
      skip += this.BATCH_SIZE;
    }

    log.info(`êµ¬ë§¤ ì‹¤íŒ¨ ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (${skip}ê°œ ë ˆì½”ë“œ)`, 'SCHEDULER', 'LOW');
  }

  /**
   * BigQueryì— ë°°ì¹˜ ë°ì´í„° ì‚½ì…
   */
  private async insertBatchToBigQuery(tableName: string, data: any[]) {
    if (data.length === 0) return;

    try {
      console.log(`ğŸ“Š Inserting data to ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      await table.insert(data);
      console.log(`âœ… Inserted ${data.length} records to ${this.DATASET_ID}.${tableName}`);
    } catch (error) {
      // ì¤‘ë³µ ë°ì´í„° ì˜¤ë¥˜ëŠ” ë¬´ì‹œ (upsert ëŒ€ì‹ )
      if (error instanceof Error && error.message?.includes('duplicate')) {
        log.warn(`ì¤‘ë³µ ë°ì´í„° ë¬´ì‹œ: ${tableName}`, 'SCHEDULER', 'LOW');
      } else {
        console.error(`âŒ Failed to insert data to ${this.DATASET_ID}.${tableName}:`, error);
        console.error(`âŒ Error details:`, error);
        throw error;
      }
    }
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì¡°íšŒ
   */
  private async getLastSyncTime(): Promise<Date | undefined> {
    // Redisë‚˜ ë³„ë„ í…Œì´ë¸”ì—ì„œ ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì¡°íšŒ
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í™˜ê²½ë³€ìˆ˜ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
    const lastSync = process.env.LAST_MONGODB_SYNC_TIME;
    return lastSync ? new Date(lastSync) : undefined;
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸
   */
  private async updateLastSyncTime(time: Date) {
    // Redisë‚˜ ë³„ë„ í…Œì´ë¸”ì— ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì €ì¥
    // ì—¬ê¸°ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” Redis ì‚¬ìš© ê¶Œì¥)
    process.env.LAST_MONGODB_SYNC_TIME = time.toISOString();
    void log.info(`ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸: ${time.toISOString()}`, 'SCHEDULER', 'LOW');
  }
}

export const mongodbSyncService = new MongodbSyncService();
