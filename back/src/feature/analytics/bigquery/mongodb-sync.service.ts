import { bigqueryClient } from './bigquery-client';
import { TABLE_SCHEMAS } from './table-schemas';
import db from 'models';
import { log } from '../../../utils/logger/logger';

export class MongodbSyncService {
  private readonly BATCH_SIZE = 1000;

  private readonly DATASET_ID = process.env.GA4_DATASET_FOUNDATION_ID!;

  /**
   * í…Œì´ë¸” ìë™ ìƒì„± í•¨ìˆ˜
   */
  private async ensureTableExists(tableName: string) {
    try {
      console.log(`ğŸ” Checking table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });

      const table = dataset.table(tableName);
      const [exists] = await table.exists();

      if (!exists) {
        const schema = TABLE_SCHEMAS[tableName];
        if (!schema) {
          throw new Error(`Schema not found for table: ${tableName}`);
        }

        console.log(`ğŸ—ï¸ Creating table ${this.DATASET_ID}.${tableName} with schema...`);
        await table.create({
          schema: schema,
          location: 'asia-northeast3',
        });
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} created successfully`);
      } else {
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} already exists`);
      }
    } catch (error) {
      console.error(`âŒ Failed to create table ${this.DATASET_ID}.${tableName}:`, error);
      throw error;
    }
  }

  /**
   * í…Œì´ë¸” ë°ì´í„°ë§Œ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
   */
  private async clearTableData(tableName: string) {
    try {
      console.log(`ğŸ—‘ï¸ Clearing data from table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      // í…Œì´ë¸” ì¡´ì¬ í™•ì¸
      const [exists] = await table.exists();

      if (exists) {
        // ë°ì´í„°ë§Œ ì‚­ì œ (í…Œì´ë¸” êµ¬ì¡°ëŠ” ìœ ì§€)
        const query = `DELETE FROM \`${this.DATASET_ID}.${tableName}\` WHERE TRUE`;
        const [job] = await bigqueryClient.createQueryJob({
          query: query,
          location: 'asia-northeast3',
        });

        await job.getQueryResults();
        console.log(`âœ… Cleared data from table ${this.DATASET_ID}.${tableName}`);
      } else {
        // í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        const schema = TABLE_SCHEMAS[tableName];
        if (!schema) {
          throw new Error(`Schema not found for table: ${tableName}`);
        }

        await table.create({
          schema: schema,
          location: 'asia-northeast3',
        });
        console.log(`âœ… Created table ${this.DATASET_ID}.${tableName}`);
      }
    } catch (error) {
      console.error(`âŒ Failed to clear table ${tableName}:`, error);
      throw error;
    }
  }

  /**
   * ëª¨ë“  ë°ì´í„°ë¥¼ ì¦ë¶„ ë™ê¸°í™”
   */
  async syncAllData() {
    try {
      log.info('MongoDB ë™ê¸°í™” ì‹œì‘', 'SCHEDULER', 'LOW');

      // jobs.tsì—ì„œ ì •ì˜ëœ MongoDB ë™ê¸°í™” ì‘ì—…ë“¤ë§Œ ì‹¤í–‰
      const { mongodbSyncJobs } = await import('../scheduler/jobs');

      const lastSync = await this.getLastSyncTime();

      // ê° ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
      for (const job of mongodbSyncJobs) {
        if (job.type === 'mongodb_sync') {
          console.log(`ğŸ”„ Processing ${job.name}...`);

          // í…Œì´ë¸” ì¡´ì¬ í™•ì¸
          await this.ensureTableExists(job.destinationTable);

          // ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
          await this.clearTableData(job.destinationTable);

          // ë°ì´í„° ë™ê¸°í™”
          await this.syncCollection(job.collection!, job.destinationTable, lastSync);

          console.log(`âœ… ${job.name} ì™„ë£Œ`);
        }
      }

      await this.updateLastSyncTime(new Date());

      log.info('MongoDB ë™ê¸°í™” ì™„ë£Œ', 'SCHEDULER', 'LOW');
    } catch (error) {
      void log.error('MongoDB ë™ê¸°í™” ì‹¤íŒ¨', 'SCHEDULER', 'HIGH', { error });
      throw error;
    }
  }

  /**
   * ë™ì ìœ¼ë¡œ ì»¬ë ‰ì…˜ ë™ê¸°í™”
   */
  private async syncCollection(collectionName: string, tableName: string, lastSyncTime?: Date) {
    const query = lastSyncTime ? { updatedAt: { $gt: lastSyncTime } } : {};

    let skip = 0;
    let hasMore = true;

    while (hasMore) {
      let data: any[];

      // ì»¬ë ‰ì…˜ë³„ë¡œ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
      switch (collectionName) {
        case 'users':
          data = await db.User.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'purchases':
          data = await db.Purchase.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'purchase_failures':
          data = await db.PurchaseFailure.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'requests':
          data = await db.Request.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        default:
          throw new Error(`Unknown collection: ${collectionName}`);
      }

      if (data.length === 0) {
        hasMore = false;
        break;
      }

      // ì»¬ë ‰ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³€í™˜ ë¡œì§ ì ìš©
      const transformedData = this.transformData(collectionName, data);

      await this.insertBatchToBigQuery(tableName, transformedData);
      skip += this.BATCH_SIZE;
    }

    log.info(`${collectionName} ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (${skip}ê°œ ë ˆì½”ë“œ)`, 'SCHEDULER', 'LOW');
  }

  /**
   * ì»¬ë ‰ì…˜ë³„ ë°ì´í„° ë³€í™˜
   */
  private transformData(collectionName: string, data: any[]): any[] {
    switch (collectionName) {
      case 'users':
        return data.map((user) => ({
          _id: user._id.toString(),
          email: user.email,
          point: Number(user.point) || 0,
          aiPoint: Number(user.aiPoint) || 0,
          level: Number(user.level) || 1,
          lastLoginAt: user.lastLoginAt?.toISOString() || null,
          MembershipAt: user.MembershipAt?.toISOString() || null,
          lastMembershipAt: user.lastMembershipAt?.toISOString() || null,
          event: user.event || null,
          createdAt: user.createdAt?.toISOString() || null,
          updatedAt: user.updatedAt?.toISOString() || null,
        }));

      case 'purchases':
        return data.map((purchase) => ({
          _id: purchase._id.toString(),
          userId: purchase.userId.toString(),
          productId: purchase.product?.productId || null,
          platform: purchase.product?.platform || null,
          type: purchase.product?.type || null,
          isExpired: purchase.isExpired || false,
          createdAt: purchase.createdAt.toISOString(),
          updatedAt: purchase.updatedAt.toISOString(),
        }));

      case 'purchase_failures':
        return data.map((failure) => ({
          _id: failure._id.toString(),
          userId: failure.userId?.toString() || null,
          productId: failure.productId?.toString() || null,
          status: failure.status || null,
          platform: failure.platform || null,
          createdAt: failure.createdAt.toISOString(),
          updatedAt: failure.updatedAt.toISOString(),
        }));

      case 'requests':
        return data.map((request) => ({
          _id: request._id.toString(),
          userId: request.userId?.toString() || null,
          status: request.status || null,
          type: request.type || null,
          name: request.name || null,
          text: request.text || null,
          product: request.product ? JSON.stringify(request.product) : null,
          review: request.review ? JSON.stringify(request.review) : null,
          answer: request.answer ? JSON.stringify(request.answer) : null,
          createdAt: request.createdAt?.toISOString() || null,
          updatedAt: request.updatedAt?.toISOString() || null,
        }));

      default:
        throw new Error(`Unknown collection: ${collectionName}`);
    }
  }

  /**
   * BigQueryì— ë°°ì¹˜ ë°ì´í„° ì‚½ì…
   */
  private async insertBatchToBigQuery(tableName: string, data: any[]) {
    if (data.length === 0) return;

    try {
      console.log(`ğŸ“Š Inserting data to ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      await table.insert(data);
      console.log(`âœ… Inserted ${data.length} records to ${this.DATASET_ID}.${tableName}`);
    } catch (error) {
      // ì¤‘ë³µ ë°ì´í„° ì˜¤ë¥˜ëŠ” ë¬´ì‹œ (upsert ëŒ€ì‹ )
      if (error instanceof Error && error.message?.includes('duplicate')) {
        log.warn(`ì¤‘ë³µ ë°ì´í„° ë¬´ì‹œ: ${tableName}`, 'SCHEDULER', 'LOW');
      } else {
        console.error(`âŒ Failed to insert data to ${this.DATASET_ID}.${tableName}:`, error);
        console.error(`âŒ Error details:`, error);
        throw error;
      }
    }
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì¡°íšŒ
   */
  private async getLastSyncTime(): Promise<Date | undefined> {
    // Redisë‚˜ ë³„ë„ í…Œì´ë¸”ì—ì„œ ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì¡°íšŒ
    // ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í™˜ê²½ë³€ìˆ˜ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
    const lastSync = process.env.LAST_MONGODB_SYNC_TIME;
    return lastSync ? new Date(lastSync) : undefined;
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸
   */
  private async updateLastSyncTime(time: Date) {
    // Redisë‚˜ ë³„ë„ í…Œì´ë¸”ì— ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì €ì¥
    // ì—¬ê¸°ì„œëŠ” í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œë¡œëŠ” Redis ì‚¬ìš© ê¶Œì¥)
    process.env.LAST_MONGODB_SYNC_TIME = time.toISOString();
    void log.info(`ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸: ${time.toISOString()}`, 'SCHEDULER', 'LOW');
  }
}

export const mongodbSyncService = new MongodbSyncService();
